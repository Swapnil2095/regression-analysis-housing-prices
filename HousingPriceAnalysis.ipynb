{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of House Prices in Ames, IA\n",
    "### Regularized Regression, Cross-Validation, and Feature Selection\n",
    "##### Grant Nikseresht, Yuqing Zhao, Yue Ning\n",
    "\n",
    "This notebook is a glimpse into the R workflow that Yuqing, Yue, and myself (Grant) used in analyzing the [Ames, IA housing dataset](https://www.kaggle.com/c/house-prices-advanced-regression-techniques) featured on Kaggle. Our analysis was done as part of our final project for Applied Stats (MATH 564) at IIT. \n",
    "\n",
    "The goal of the Kaggle challenge was to predict the selling price of a home given a number of its attributes. Selecting which features to use in our model was the primary challenge in analyzing this dataset, where many of the features are collinear, trivial, or uncorrelated with the dependent variable. \n",
    "\n",
    "For our course project, we decided to compare different methods for feature selection including manual selection, stepwise regression, and regularized regression. In this notebook, we'll explore the dataset, implement several regression methods with cross-validation, and compare some results. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(rpart)\n",
    "library(caret)\n",
    "library(leaps)\n",
    "library(glmnet)\n",
    "library(ggplot2)\n",
    "library(tabplot)\n",
    "library(reshape2)\n",
    "options(warn=-1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the Data\n",
    "\n",
    "We performed some preprocessing of the original Kaggle data and stored it in a file in the `data` folder. Let's load it into a dataframe and pull the index out to prepare it for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df <- read.csv(\"./data/train_processed.csv\")\n",
    "train_df <- train_df[,-c(1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our processed dataset now contains 1457 observations and 51 explanatory variables. All missing and nonsensical values have been removed. Let's take a look at the data using some handy R functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dim(train_df)\n",
    "summary(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a quick visual glimpse at the distribution of the log of selling price. We use both the built in R functions like `boxplot` and the more extensive plotting package `ggplot2`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxplot(train_df$SalePrice)\n",
    "ggplot(data=train_df, aes(train_df$SalePrice)) + \n",
    "  geom_histogram (col=\"red\", aes(fill=..count..)) +\n",
    "  scale_fill_gradient(\"Count\", low = \"green\", high = \"red\")+\n",
    "  labs(title=\"Histogram for SalePrice\") +\n",
    "  labs(x=\"SalePrice\", y=\"Count\")+\n",
    "  theme(plot.title = element_text(hjust = 0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a 10,000 foot view of the explanatory variables as well using `tabplot`. A few are shown below, but feel free to try this on other variables. This view proves useful for visualizing how homoegenous or incomplete a dataset is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tableplot(train_df[,30:34])\n",
    "tableplot(train_df[,41:45])\n",
    "tableplot(train_df[,5:9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an example of an issue in the data that required some preprocessing. Each component of the house generally had a few associated explanatory variables. For instance, there are several variables each providing similar information about basements and garages. The `xtabs` function below shows a contingency table to estimate the amount of collinearity between factor variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "xtabs(~GarageQual+GarageCond+GarageFinish, data=train_df)\n",
    "xtabs(~BsmtCond+BsmtFinType1, data=train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfectly collinear or homogenous variables leading can sabotage regression variables, so we're only going to keep one of the variables for garage and one for basement. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df <- train_df[, -c(which(colnames(train_df) == \"GarageFinish\"),\n",
    "                            which(colnames(train_df) == \"Exterior2nd\"),\n",
    "                            which(colnames(train_df) == \"GarageCond\"),\n",
    "                            which(colnames(train_df) == \"BsmtFinType1\"))]\n",
    "dim(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a function we'll use at the end to compute our error. We're using root mean squared logarithmic error (RMSLE) to compare models. We've already log transformed our dependent variable, so this is basically just RMSE.\n",
    "\n",
    "$RMSLE = \\sqrt{(\\frac{1}{n}\\sum_{i=1}^n(\\hat{Y} - Y)^2)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmsle <- function(yhat, y) {\n",
    "  n <- length(yhat)\n",
    "  return(sqrt((1/n)*sum((yhat-y)^2)))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "Since the only labeled data we have access to is the training set, we're going to use cross-validation to estimate how well our model will generalize to new data. \n",
    "\n",
    "For this, we're going to use the `caret` package, which provides an interface for cross-validating models on a variety of methods. A control parameter is initialized below that will tell future calls to `caret` what settings to use for performing cross-validation. We used 10 folds in our analysis, but we'll only use 3 here so training will finish in a reasonable time. Feel free to increase the number if you're trying this on your own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(564)\n",
    "controlParameter <- trainControl(method = \"cv\",\n",
    "                                  number = 3,\n",
    "                                  savePredictions = TRUE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_ols <- train(SalePrice~.,\n",
    "                data = train_df,\n",
    "                method='lm',\n",
    "                trControl=controlParameter)\n",
    "ols_fit <- lm_ols$finalModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "summary(ols_fit)\n",
    "plot(ols_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_cats <- train(SalePrice~TotBathrooms+SaleCondition+GarageArea+\n",
    "                   KitchenQual+GrLivArea+TotalBsmtSF+OverallCond+OverallQual+\n",
    "                   BldgType+Condition1+MSZoning,\n",
    "                 data=train_df, \n",
    "                 method=\"lm\",\n",
    "                 trControl=controlParameter)\n",
    "cats_fit <- lm_cats$finalModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(cats_fit)\n",
    "plot(cats_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_forward <- train(SalePrice~., \n",
    "                    data=train_df,\n",
    "                    method='leapForward',\n",
    "                    trControl=controlParameter,\n",
    "                    tuneGrid = expand.grid(nvmax = seq(1, 180, 1)))\n",
    "fwd_fit <- lm_forward$finalModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_forward$results[lm_forward$bestTune[1,1],]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_backward <- train(SalePrice~., \n",
    "                     data=train_df,\n",
    "                     method='leapBackward', \n",
    "                     trControl=controlParameter, \n",
    "                     tuneGrid = expand.grid(nvmax = seq(1, 180, 1)))\n",
    "bwd_fit <- lm_backward$finalModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas <- 10^seq(-1, -5, length = 100) # This NaNs after like 400\n",
    "ridgeGrid <- expand.grid(alpha=0,lambda=lambdas)\n",
    "lm_ridge <- train(SalePrice~., data=train_df, method = 'glmnet', trControl=controlParameter, tuneGrid=ridgeGrid)\n",
    "ridge_fit <- lm_ridge$finalModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas <- 10^seq(-2, -5, length = 300) # Opt lambda probably between .00001 and .01\n",
    "lassoGrid <- expand.grid(alpha=1,lambda=lambdas)\n",
    "lm_lasso <- train(SalePrice~., data=train_df, method = 'glmnet', trControl=controlParameter, tuneGrid=lassoGrid)\n",
    "lasso_fit <- lm_lasso$finalModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elasGrid <- expand.grid(alpha=seq(0, 1, length=21),lambda=lambdas)\n",
    "lm_elas <- train(SalePrice~., data=train_df, method = 'glmnet', trControl=controlParameter, tuneGrid=elasGrid)\n",
    "elas_fit <- lm_elas$finalModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "treeGrid <- expand.grid(cp=10^seq(-5,-3, length=101))\n",
    "tree_cp <- train(SalePrice~.,\n",
    "                 data=train_df,\n",
    "                 method='rpart',\n",
    "                 trControl=controlParameter,\n",
    "                 tuneGrid=treeGrid)\n",
    "# Zeroing in on the optimal value\n",
    "treeFineGrid <- expand.grid(cp=seq(0.0002,.0004, length=101))\n",
    "tree_cp <- train(SalePrice~.,\n",
    "                 data=train_df,\n",
    "                 method='rpart',\n",
    "                 trControl=controlParameter,\n",
    "                 tuneGrid=treeFineGrid)\n",
    "tree_fit <- tree_cp$finalModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_ols_pred <- predict(lm_ols,train_df)\n",
    "lm_cats_pred <- predict(lm_cats,train_df)\n",
    "lm_forward_pred <- predict(lm_forward,train_df)\n",
    "lm_backward_pred <- predict(lm_backward,train_df)\n",
    "lm_lasso_pred <- predict(lm_lasso,train_df)\n",
    "lm_ridge_pred <- predict(lm_ridge,train_df)\n",
    "lm_elas_pred <- predict(lm_elas,train_df)\n",
    "tree_cp_pred <- predict(tree_cp,train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ols_res <- ols_fit$residuals\n",
    "cats_res <- cats_fit$residuals\n",
    "fwd_res <- lm_forward_pred - train_df$SalePrice\n",
    "bwd_res <- lm_backward_pred - train_df$SalePrice\n",
    "lasso_res <- lm_lasso_pred - train_df$SalePrice\n",
    "ridge_res <- lm_ridge_pred - train_df$SalePrice\n",
    "elas_res <- lm_elas_pred - train_df$SalePrice\n",
    "tree_res <- tree_cp_pred - train_df$SalePrice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_rmsle <- rmsle(abs(lm_ols_pred), train_df$SalePrice)\n",
    "lm_cats_rmsle <- rmsle(abs(lm_cats_pred), train_df$SalePrice)\n",
    "lm_forward_rmsle <- rmsle(abs(lm_forward_pred), train_df$SalePrice) \n",
    "lm_backward_rmsle <- rmsle(abs(lm_backward_pred), train_df$SalePrice)\n",
    "lm_lasso_rmsle <- rmsle(abs(lm_lasso_pred), train_df$SalePrice)\n",
    "lm_ridge_rmsle <- rmsle(abs(lm_ridge_pred), train_df$SalePrice)\n",
    "lm_elas_rmsle <- rmsle(abs(lm_elas_pred), train_df$SalePrice)\n",
    "tree_cp_rmsle <- rmsle(abs(tree_cp_pred), train_df$SalePrice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmsle_scores <- c(lm_rmsle, lm_cats_rmsle, lm_forward_rmsle,\n",
    "                  lm_backward_rmsle, lm_ridge_rmsle, lm_lasso_rmsle,\n",
    "                  lm_elas_rmsle, tree_cp_rmsle)\n",
    "names(rmsle_scores) <- c(\"OLS_Full\", \"OLS_Manual\", \"OLS_Forward\",\n",
    "                         \"OLS_Backward\", \"Ridge\", \"LASSO\",\n",
    "                         \"Elastic\",\"Tree_CP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_lm_ols <- lm_ols$results[as.numeric(rownames(lm_ols$bestTune)),]\n",
    "best_lm_cats <- lm_cats$results[as.numeric(rownames(lm_cats$bestTune)),]\n",
    "best_lm_forward <- lm_forward$results[as.numeric(rownames(lm_forward$bestTune)),]\n",
    "best_lm_backward <- lm_backward$results[as.numeric(rownames(lm_backward$bestTune)),]\n",
    "best_lm_ridge <- lm_ridge$results[as.numeric(rownames(lm_ridge$bestTune)),]\n",
    "best_lm_lasso <- lm_lasso$results[as.numeric(rownames(lm_lasso$bestTune)),]\n",
    "best_lm_elastic <- lm_elas$results[as.numeric(rownames(lm_elas$bestTune)),]\n",
    "best_tree_cp <- tree_cp$results[as.numeric(rownames(tree_cp$bestTune)),]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results <- data.frame(method = names(rmsle_scores), \n",
    "                         rmse = c(best_lm_ols['RMSE'][1,1],\n",
    "                                  best_lm_cats['RMSE'][1,1],\n",
    "                                  best_lm_forward['RMSE'][1,1],\n",
    "                                  best_lm_backward['RMSE'][1,1],\n",
    "                                  best_lm_ridge['RMSE'][1,1],\n",
    "                                  best_lm_lasso['RMSE'][1,1],\n",
    "                                  best_lm_elastic['RMSE'][1,1],\n",
    "                                  best_tree_cp['RMSE'][1,1]),\n",
    "                         rmse_sd = c(best_lm_ols['RMSESD'][1,1],\n",
    "                                      best_lm_cats['RMSESD'][1,1],\n",
    "                                      best_lm_forward['RMSESD'][1,1],\n",
    "                                      best_lm_backward['RMSESD'][1,1],\n",
    "                                      best_lm_ridge['RMSESD'][1,1],\n",
    "                                      best_lm_lasso['RMSESD'][1,1],\n",
    "                                      best_lm_elastic['RMSESD'][1,1],\n",
    "                                      best_tree_cp['RMSESD'][1,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ggplot(cv_results, aes(x=method, y=rmse)) + \n",
    "         geom_dotplot(binaxis = 'y', stackdir = 'center') +\n",
    "         geom_errorbar(aes(ymin=rmse-rmse_sd, ymax=rmse+rmse_sd), width=.2,\n",
    "                                  position=position_dodge(.0)) +\n",
    "         xlab(\"Method\") +\n",
    "         ylab(\"Cross-Validation RMSE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals <- data.frame(id = seq(1, length(ols_res)),\n",
    "                        OLS_Full=ols_res,\n",
    "                        OLS_Manual=cats_res,\n",
    "                        OLS_Forward=fwd_res,\n",
    "                        OLS_Backward=bwd_res,\n",
    "                        Ridge=ridge_res,\n",
    "                        LASSO=lasso_res,\n",
    "                        Elastic=elas_res,\n",
    "                        Tree=tree_res)\n",
    "res_melt <- melt(residuals, id.vars = \"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ggplot(res_melt, aes(x=id, y=value, color=variable)) + \n",
    "  geom_point(alpha=0.3, size=0.75) +\n",
    "  scale_colour_manual(values=c(\"red\", \"blue\", \"green\", \"orange\",\n",
    "                               \"gray\", \"brown\", \"black\", \"purple\")) +\n",
    "  xlab(\"Observation Index\") +\n",
    "  ylab(\"Residual Value\") +\n",
    "  scale_fill_discrete(name = \"Model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
