{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of House Prices in Ames, IA\n",
    "### Regularized Regression, Cross-Validation, and Feature Selection\n",
    "##### Grant Nikseresht, Yuqing Zhao, Yue Ning\n",
    "\n",
    "This notebook is a glimpse into the R workflow that Yuqing, Yue, and myself (Grant) used in analyzing the [Ames, IA housing dataset](https://www.kaggle.com/c/house-prices-advanced-regression-techniques) featured on Kaggle. Our analysis was done as part of our final project for Applied Stats (MATH 564) at IIT. \n",
    "\n",
    "The goal of the Kaggle challenge was to predict the selling price of a home given a number of its attributes. Selecting which features to use in our model was the primary challenge in analyzing this dataset, where many of the features are collinear, trivial, or uncorrelated with the dependent variable. \n",
    "\n",
    "For our course project, we decided to compare different methods for feature selection including manual selection, stepwise regression, and regularized regression. In this notebook, we'll explore the dataset, implement several regression methods with cross-validation, and compare some results. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "library(rpart)\n",
    "library(caret)\n",
    "library(leaps)\n",
    "library(glmnet)\n",
    "library(ggplot2)\n",
    "library(tabplot)\n",
    "library(reshape2)\n",
    "options(warn=-1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the Data\n",
    "\n",
    "We performed some preprocessing of the original Kaggle data and stored it in a file in the `data` folder. Let's load it into a dataframe and pull the index out to prepare it for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df <- read.csv(\"./data/train_processed.csv\")\n",
    "train_df <- train_df[,-c(1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our processed dataset now contains 1457 observations and 51 explanatory variables. All missing and nonsensical values have been removed. Let's take a look at the data using some handy R functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dim(train_df)\n",
    "summary(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a quick visual glimpse at the distribution of the log of selling price. We use both the built in R functions like `boxplot` and the more extensive plotting package `ggplot2`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "boxplot(train_df$SalePrice)\n",
    "ggplot(data=train_df, aes(train_df$SalePrice)) + \n",
    "  geom_histogram (col=\"red\", aes(fill=..count..)) +\n",
    "  scale_fill_gradient(\"Count\", low = \"green\", high = \"red\")+\n",
    "  labs(title=\"Histogram for SalePrice\") +\n",
    "  labs(x=\"SalePrice\", y=\"Count\")+\n",
    "  theme(plot.title = element_text(hjust = 0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a 10,000 foot view of the explanatory variables as well using `tabplot`. A few are shown below, but feel free to try this on other variables. This view proves useful for visualizing how homoegenous or incomplete a dataset is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tableplot(train_df[,30:34])\n",
    "tableplot(train_df[,41:45])\n",
    "tableplot(train_df[,5:9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an example of an issue in the data that required some preprocessing. Each component of the house generally had a few associated explanatory variables. For instance, there are several variables each providing similar information about basements and garages. The `xtabs` function below shows a contingency table to estimate the amount of collinearity between factor variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "xtabs(~GarageQual+GarageCond+GarageFinish, data=train_df)\n",
    "xtabs(~BsmtCond+BsmtFinType1, data=train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfectly collinear or homogenous variables leading can sabotage regression variables, so we're only going to keep one of the variables for garage and one for basement. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_df <- train_df[, -c(which(colnames(train_df) == \"GarageFinish\"),\n",
    "                            which(colnames(train_df) == \"Exterior2nd\"),\n",
    "                            which(colnames(train_df) == \"GarageCond\"),\n",
    "                            which(colnames(train_df) == \"BsmtFinType1\"))]\n",
    "dim(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a function we'll use at the end to compute our error. We're using root mean squared logarithmic error (RMSLE) to compare models. We've already log transformed our dependent variable, so this is basically just RMSE.\n",
    "\n",
    "$RMSLE = \\sqrt{(\\frac{1}{n}\\sum_{i=1}^n(\\hat{Y} - Y)^2)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rmsle <- function(yhat, y) {\n",
    "  n <- length(yhat)\n",
    "  return(sqrt((1/n)*sum((yhat-y)^2)))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "Since the only labeled data we have access to is the training set, we're going to use cross-validation to estimate how well our model will generalize to new data. \n",
    "\n",
    "Cross-validation involves splitting a dataset into $k$ different folds (or partitions) of equal size and performing $k$ iterations of model training. On each iteration, $k-1$ folds are selected to make up the training set with the last fold held out as the test set. The model is trained on the training set and then RMSE is estimated on the held out fold. The cross-validation error is the average of the hold-out errors. The RMSE reported here will be the cross-validated RMSE which is given by,\n",
    "\n",
    "$$RMSE_{CV} =\\frac{1}{k} \\sum_{i=1}^{k} \\sqrt{MSE_{i}}$$\n",
    "\n",
    "We're going to use the `caret` package, which provides an interface for cross-validating models on a variety of methods. A control parameter is initialized below that will tell future calls to `caret` what settings to use for performing cross-validation. We used 10 folds in our analysis, but we'll only use 3 here so training will finish in a reasonable time. Feel free to increase the number if you're trying this on your own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "set.seed(564)\n",
    "controlParameter <- trainControl(method = \"cv\",\n",
    "                                  number = 3,\n",
    "                                  savePredictions = TRUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ordinary Least Squares Regression (OLS)\n",
    "\n",
    "We start with an OLS model containing all of the explanatory variables to establish a performance baseline. The OLS model is given by $y=\\beta_0+x_1\\beta_1+x_2\\beta_2+\\dots+x_k\\beta_k + \\epsilon$. The objective is to select the $\\beta$ coefficients to minimize the total error over the training set.\n",
    "\n",
    "We use `caret`'s function `train` which is a general purpose training function that works with a variety of models. `train` takes in the control parameters we defined above and performs training in a cross-validated way. \n",
    "\n",
    "After training our model, we'll extract the final model which extracts the model with the best performance across the given parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lm_ols <- train(SalePrice~.,\n",
    "                data = train_df,\n",
    "                method='lm',\n",
    "                trControl=controlParameter)\n",
    "ols_fit <- lm_ols$finalModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R has a couple of built-in functions that can produce useful results when applied to a variety of objects. \n",
    "\n",
    "The `summary` function will produce an overview of a trained model in an easy to read form. The `plot` function will sometimes generate plots related to a mdoel when you pass it the model object. When I try a new package, I'll often just try applying these functions to see what (if anything) is produced. You can see the output for our `caret` model below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "summary(ols_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot(ols_fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The baseline model performance is shown below in terms of RMSE and $R^2$. All later models will be judged by whether\n",
    "or not they improve on these numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lm_ols$results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manual Feature Selection OLS\n",
    "\n",
    "For the next model, we applied some heuristics to reduce the number of explanatory variables to one per 'category'. We take categories to be each of the different components of a house such as the bathroom, garage, or zoning type. We keep only the most significant variable in each general category from the first model. The motivation here was to try to build a model with a smaller amount of multicollinearity using some manual selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lm_cats <- train(SalePrice~TotBathrooms+SaleCondition+GarageArea+\n",
    "                   KitchenQual+GrLivArea+TotalBsmtSF+OverallCond+OverallQual+\n",
    "                   BldgType+Condition1+MSZoning,\n",
    "                 data=train_df, \n",
    "                 method=\"lm\",\n",
    "                 trControl=controlParameter)\n",
    "cats_fit <- lm_cats$finalModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "summary(cats_fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first naive approach was not enough to improve performance and actually decreased model performance quite a bit. In other words, there is quite a bit of useful information contained in some of the collinear variables, so we need a more nuanced method for feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lm_cats$results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forward Stepwise Regression\n",
    "\n",
    "The first automatic method for feature selection we'll try is forward stepwise regression. This method begins with the null model containing no variables and systematically adds one variable at a time. This is carried out, generally, until there are no variables that can be added that improve on some set criterion. Here we use the default criterion, BIC. \n",
    "\n",
    "On each iteration, the current model is refit with each of the remaining variables. Amongst all the new models (and the current model), the one with the lowest BIC score is chosen as the model for the next iteration. \n",
    "\n",
    "This is our first method fit using `caret`'s parameter tuning feature. Forward selection depends on the maximize size of model available given by the parameter `nvmax`. We try forward selection for model sizes up to 180 variables (recall that factor variables must be turned into binary variables, increasing the actual number of explanatory variables). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lm_forward <- train(SalePrice~., \n",
    "                    data=train_df,\n",
    "                    method='leapForward',\n",
    "                    trControl=controlParameter,\n",
    "                    tuneGrid = expand.grid(nvmax = seq(1, 180, 1))) # set a grid of parameters to try\n",
    "fwd_fit <- lm_forward$finalModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lm_forward$results[lm_forward$bestTune[1,1],]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backward Stepwise Regression\n",
    "\n",
    "Backward stepwise regression is the inverse of forward stepwise regression - start with the full model and start removing variables. We use the same tuning grid here as with forward stepwise regression, but get different results.\n",
    "\n",
    "It's worth noting that there are $2^k$ possible models where $k$ is the number of explanatory variables, so trying every possible model and picking the best one is often impossible. In our case with around 180 explanatory variables, there are $1.532 x 10^{54}$ possible models for us to choose from. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lm_backward <- train(SalePrice~., \n",
    "                     data=train_df,\n",
    "                     method='leapBackward', \n",
    "                     trControl=controlParameter, \n",
    "                     tuneGrid = expand.grid(nvmax = seq(1, 180, 1)))\n",
    "bwd_fit <- lm_backward$finalModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `caret` training object provides a great visualization of parameter tuning by default using the `plot` function. Here, we can see how the $RMSE_{CV}$ score changes with the number of predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot(lm_backward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularized Regression\n",
    "\n",
    "A slight expansion of the linear regression model formulation can help to solve our problem of multicollinearity in the dataset and improve the cross-validation error. Rather than using RMSE on its own, the objective function is expanded to include two penalization terms. This new objective function is below where $\\lambda$ and $\\alpha$ are parameters to be selected. \n",
    "\n",
    "$$\\min_{\\beta_0, \\beta} \\frac{1}{N} \\sum_{i=1}^N RMSE + \\frac{\\lambda[(1-\\alpha)||{\\beta}_2^2||}{2} + \\alpha||\\beta_1||$$\n",
    "\n",
    "The effect of altering the objective function amounts to 'shrinking' the $\\beta$ coefficients for larger values of $\\lambda$. Most importantly, $\\beta$ coefficients for insignificant variables are shrunken to zero (or near zero) for insignificant factors. In this sense, regularized regression methods are also useful for feature selection. \n",
    "\n",
    "Here we'll compare the three prevailing regularization techniques, which each make different assumptions in the objective function.\n",
    "\n",
    "#### Ridge Regression\n",
    "\n",
    "Ridge regression is the case where $\\alpha = 0$. We'll use `caret` here to select an optimal value of $\\lambda$. We test values between 0.1 and 0.00001.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lambdas <- 10^seq(-1, -5, length = 100) \n",
    "ridgeGrid <- expand.grid(alpha=0,lambda=lambdas)\n",
    "lm_ridge <- train(SalePrice~., data=train_df, method = 'glmnet', trControl=controlParameter, tuneGrid=ridgeGrid)\n",
    "ridge_fit <- lm_ridge$finalModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lm_ridge$results[as.numeric(rownames(lm_ridge$bestTune)),]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot(lm_ridge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following plot shows how certain variables weights are shrunk across values of the L1 norm. Variables that are quickly shrunk towards zero are considered less important and 'removed' by the regularization method. Each color in the plot is a different variable, although we have too many variables here to distinguish between them. Nonetheless, it's a good visualization of what's going on in regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot(ridge_fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LASSO Regression\n",
    "\n",
    "LASSO is the case where $\\alpha=1$. We use a similar, but more fine-grained, grid of $\\lambda$ values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lambdas <- 10^seq(-2, -5, length = 300) \n",
    "lassoGrid <- expand.grid(alpha=1,lambda=lambdas)\n",
    "lm_lasso <- train(SalePrice~., data=train_df, method = 'glmnet', trControl=controlParameter, tuneGrid=lassoGrid)\n",
    "lasso_fit <- lm_lasso$finalModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lm_lasso$results[as.numeric(rownames(lm_lasso$bestTune)),]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot(lm_lasso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot(lasso_fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ElasticNet Regression\n",
    "\n",
    "ElasticNet regression gives partial weight to both the ridge and LASSO penalties. This amounts to varying $\\alpha$ between 0 and 1. In this case, we need to tune both $\\alpha$ and $\\lambda$, leading to a greater computation time, but hopefully a more fine tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "elasGrid <- expand.grid(alpha=seq(0, 1, length=21),lambda=lambdas)\n",
    "lm_elas <- train(SalePrice~., data=train_df, method = 'glmnet', trControl=controlParameter, tuneGrid=elasGrid)\n",
    "elas_fit <- lm_elas$finalModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lm_elas$results[as.numeric(rownames(lm_elas$bestTune)),]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With two parameters to tune, you can see in the following graph how RMSE varies over both of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot(lm_elas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tree Regression\n",
    "\n",
    "One final method we decided to try was tree regression. This is a non-parametric method that searches for an optimal series of splits in explanatory variables to estimate the dependent variable. This class of methods is generally known as decision trees, and it will become clear here that they're not well suited for this problem. \n",
    "\n",
    "We optimize trees over a complexity paramter, which constrains the size of final trees. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "treeGrid <- expand.grid(cp=10^seq(-5,-3, length=101))\n",
    "tree_cp <- train(SalePrice~.,\n",
    "                 data=train_df,\n",
    "                 method='rpart',\n",
    "                 trControl=controlParameter,\n",
    "                 tuneGrid=treeGrid)\n",
    "# Zeroing in on the optimal value\n",
    "treeFineGrid <- expand.grid(cp=seq(0.0002,.0004, length=101))\n",
    "tree_cp <- train(SalePrice~.,\n",
    "                 data=train_df,\n",
    "                 method='rpart',\n",
    "                 trControl=controlParameter,\n",
    "                 tuneGrid=treeFineGrid)\n",
    "tree_fit <- tree_cp$finalModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot(tree_cp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tree_cp$results[as.numeric(rownames(tree_cp$bestTune)),]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of Methods\n",
    "\n",
    "In this section, we'll summarize and compare the results. We're interested in seeing whether or not regularization methods are better tools for feature selection than stepwise regression or OLS. \n",
    "\n",
    "First, let's gather predictions, $\\hat{y}$ over the entire dataset for each method using the `predict` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lm_ols_pred <- predict(lm_ols,train_df)\n",
    "lm_cats_pred <- predict(lm_cats,train_df)\n",
    "lm_forward_pred <- predict(lm_forward,train_df)\n",
    "lm_backward_pred <- predict(lm_backward,train_df)\n",
    "lm_lasso_pred <- predict(lm_lasso,train_df)\n",
    "lm_ridge_pred <- predict(lm_ridge,train_df)\n",
    "lm_elas_pred <- predict(lm_elas,train_df)\n",
    "tree_cp_pred <- predict(tree_cp,train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also maintain the residuals for diagnostics later. Patterns in the residuals could clue us in on systematic errors in our models that we may need to correct in future work. Residuals are the difference between our model's prediction and the actual value, $\\hat{y} - y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ols_res <- ols_fit$residuals\n",
    "cats_res <- cats_fit$residuals\n",
    "fwd_res <- lm_forward_pred - train_df$SalePrice\n",
    "bwd_res <- lm_backward_pred - train_df$SalePrice\n",
    "lasso_res <- lm_lasso_pred - train_df$SalePrice\n",
    "ridge_res <- lm_ridge_pred - train_df$SalePrice\n",
    "elas_res <- lm_elas_pred - train_df$SalePrice\n",
    "tree_res <- tree_cp_pred - train_df$SalePrice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's apply our `rmsle` function we wrote earlier by inputting the predictions and actual values of the sale price. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lm_rmsle <- rmsle(abs(lm_ols_pred), train_df$SalePrice)\n",
    "lm_cats_rmsle <- rmsle(abs(lm_cats_pred), train_df$SalePrice)\n",
    "lm_forward_rmsle <- rmsle(abs(lm_forward_pred), train_df$SalePrice) \n",
    "lm_backward_rmsle <- rmsle(abs(lm_backward_pred), train_df$SalePrice)\n",
    "lm_lasso_rmsle <- rmsle(abs(lm_lasso_pred), train_df$SalePrice)\n",
    "lm_ridge_rmsle <- rmsle(abs(lm_ridge_pred), train_df$SalePrice)\n",
    "lm_elas_rmsle <- rmsle(abs(lm_elas_pred), train_df$SalePrice)\n",
    "tree_cp_rmsle <- rmsle(abs(tree_cp_pred), train_df$SalePrice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rmsle_scores <- c(lm_rmsle, lm_cats_rmsle, lm_forward_rmsle,\n",
    "                  lm_backward_rmsle, lm_ridge_rmsle, lm_lasso_rmsle,\n",
    "                  lm_elas_rmsle, tree_cp_rmsle)\n",
    "names(rmsle_scores) <- c(\"OLS_Full\", \"OLS_Manual\", \"OLS_Forward\",\n",
    "                         \"OLS_Backward\", \"Ridge\", \"LASSO\",\n",
    "                         \"Elastic\",\"Tree_CP\")\n",
    "rmsle_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These scores come from applying the trained models to the training set. The number we're truly interested in is the generalization error that we estimated with cross-validation. Let's summarize the cross-validation errors below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_lm_ols <- lm_ols$results[as.numeric(rownames(lm_ols$bestTune)),]\n",
    "best_lm_cats <- lm_cats$results[as.numeric(rownames(lm_cats$bestTune)),]\n",
    "best_lm_forward <- lm_forward$results[as.numeric(rownames(lm_forward$bestTune)),]\n",
    "best_lm_backward <- lm_backward$results[as.numeric(rownames(lm_backward$bestTune)),]\n",
    "best_lm_ridge <- lm_ridge$results[as.numeric(rownames(lm_ridge$bestTune)),]\n",
    "best_lm_lasso <- lm_lasso$results[as.numeric(rownames(lm_lasso$bestTune)),]\n",
    "best_lm_elastic <- lm_elas$results[as.numeric(rownames(lm_elas$bestTune)),]\n",
    "best_tree_cp <- tree_cp$results[as.numeric(rownames(tree_cp$bestTune)),]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv_results <- data.frame(method = names(rmsle_scores), \n",
    "                         rmse = c(best_lm_ols['RMSE'][1,1],\n",
    "                                  best_lm_cats['RMSE'][1,1],\n",
    "                                  best_lm_forward['RMSE'][1,1],\n",
    "                                  best_lm_backward['RMSE'][1,1],\n",
    "                                  best_lm_ridge['RMSE'][1,1],\n",
    "                                  best_lm_lasso['RMSE'][1,1],\n",
    "                                  best_lm_elastic['RMSE'][1,1],\n",
    "                                  best_tree_cp['RMSE'][1,1]),\n",
    "                         rmse_sd = c(best_lm_ols['RMSESD'][1,1],\n",
    "                                      best_lm_cats['RMSESD'][1,1],\n",
    "                                      best_lm_forward['RMSESD'][1,1],\n",
    "                                      best_lm_backward['RMSESD'][1,1],\n",
    "                                      best_lm_ridge['RMSESD'][1,1],\n",
    "                                      best_lm_lasso['RMSESD'][1,1],\n",
    "                                      best_lm_elastic['RMSESD'][1,1],\n",
    "                                      best_tree_cp['RMSESD'][1,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot below shows the cross-validated RMSE for each method along with standard deviation bars. Large bars indicate a big spread in the errors computed at each iteration of cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ggplot(cv_results, aes(x=method, y=rmse)) + \n",
    "         geom_dotplot(binaxis = 'y', stackdir = 'center') +\n",
    "         geom_errorbar(aes(ymin=rmse-rmse_sd, ymax=rmse+rmse_sd), width=.2,\n",
    "                                  position=position_dodge(.0)) +\n",
    "         xlab(\"Method\") +\n",
    "         ylab(\"Cross-Validation RMSE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now compile the residuals into a dataframe, so we can plot them to look for patterns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "residuals <- data.frame(id = seq(1, length(ols_res)),\n",
    "                        OLS_Full=ols_res,\n",
    "                        OLS_Manual=cats_res,\n",
    "                        OLS_Forward=fwd_res,\n",
    "                        OLS_Backward=bwd_res,\n",
    "                        Ridge=ridge_res,\n",
    "                        LASSO=lasso_res,\n",
    "                        Elastic=elas_res,\n",
    "                        Tree=tree_res)\n",
    "res_melt <- melt(residuals, id.vars = \"id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saw earlier how to look at some standardized residual plots using R's automatic plotting features on a model. Below, we use `ggplot` to see how residuals varied across observations. This reveals some of the outliers and how different models predicted them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ggplot(res_melt, aes(x=id, y=value, color=variable)) + \n",
    "  geom_point(alpha=0.3, size=0.75) +\n",
    "  scale_colour_manual(values=c(\"red\", \"blue\", \"green\", \"orange\",\n",
    "                               \"gray\", \"brown\", \"black\", \"purple\")) +\n",
    "  xlab(\"Observation Index\") +\n",
    "  ylab(\"Residual Value\") +\n",
    "  scale_fill_discrete(name = \"Model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our analysis showed that regularization methods can improve feature selection in regression problems with many collinear variables. Here's a couple of exercises you could try after this workshop to get more acquainted with R:\n",
    "\n",
    "1. Try increasing the number of folds and seeing what happens. 3 folds is not quite enough to get a robust estimate of cross-validation error. For our analysis, we used 10 folds, but for methods like ElasticNet this can get intensive. \n",
    "\n",
    "2. We used a preprocessed dataset here, but I've included the unprocessed dataset in the /data folder in the Git repo. Try starting from that dataset and implementing the same models. See what problems come up and tinker with the dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
